{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Gutierya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/Gutierya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Gutierya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "from IPython.core.display import display\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from time import time\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data sorting 1/2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non clickbait Data: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                             feature  target\n0  Bill Changing Credit Card Rules Is Sent to Oba...     0.0\n1  In Hollywood, the Easy-Money Generation Toughe...     0.0\n2  1700 runners still unaccounted for in UK's Lak...     0.0\n3  Yankees Pitchers Trade Fielding Drills for Put...     0.0\n4  Large earthquake rattles Indonesia; Seventh in...     0.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Bill Changing Credit Card Rules Is Sent to Oba...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>In Hollywood, the Easy-Money Generation Toughe...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1700 runners still unaccounted for in UK's Lak...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Yankees Pitchers Trade Fielding Drills for Put...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Large earthquake rattles Indonesia; Seventh in...</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clickbait Data: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                             feature  target\n0                                 Should I Get Bings     1.0\n1      Which TV Female Friend Group Do You Belong In     1.0\n2  The New \"Star Wars: The Force Awakens\" Trailer...     1.0\n3  This Vine Of New York On \"Celebrity Big Brothe...     1.0\n4  A Couple Did A Stunning Photo Shoot With Their...     1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Should I Get Bings</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Which TV Female Friend Group Do You Belong In</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Data: \n"
     ]
    },
    {
     "data": {
      "text/plain": "                                             feature  target\n0                                 Should I Get Bings     1.0\n1      Which TV Female Friend Group Do You Belong In     1.0\n2  The New \"Star Wars: The Force Awakens\" Trailer...     1.0\n3  This Vine Of New York On \"Celebrity Big Brothe...     1.0\n4  A Couple Did A Stunning Photo Shoot With Their...     1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>feature</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Should I Get Bings</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Which TV Female Friend Group Do You Belong In</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pulling in non clickbait txt file\n",
    "raw_nonclick = pd.read_csv('./data/non_clickbait_data.txt',\n",
    "                   sep='delimiter', header=None,\n",
    "                   names=['feature', 'target'])\n",
    "#adding float 0 'false' value to non clickbait df\n",
    "target_nonclick = raw_nonclick['target'] = 0.0\n",
    "#peeking at df table\n",
    "print('Non clickbait Data: ')\n",
    "display(raw_nonclick.head())\n",
    "#pulling in clickbait txt file\n",
    "raw_click = pd.read_csv('./data/clickbait_data.txt',\n",
    "                   sep='delimiter', header=None,\n",
    "                   names=['feature', 'target'])\n",
    "#adding float 1 'true' value to non clickbait df\n",
    "target_click = raw_click['target'] = 1.0\n",
    "#peeking at df table\n",
    "print('Clickbait Data: ')\n",
    "display(raw_click.head())\n",
    "\n",
    "#Merging 2 files into one\n",
    "print('Merged Data: ')\n",
    "merged = pd.concat([raw_click, raw_nonclick], ignore_index=True, sort=True)\n",
    "display(merged.head())\n",
    "\n",
    "#Creating two variables for feature and target\n",
    "X = merged['feature']\n",
    "Y = merged['target']\n",
    "\n",
    "\n",
    "#Optionally instead of zeroes and ones for target values, using strings:\n",
    "    # \"clickbait\" vs \"nonclickbait\"\n",
    "#creating copies of previous df's\n",
    "raw_nonclick_copy = raw_nonclick\n",
    "raw_click_copy = raw_click\n",
    "#assigning string values to 'target' column for both txt files\n",
    "y_str_nonclick = raw_nonclick_copy['target'] = \"not clickbait\"\n",
    "y_str_click = raw_click_copy['target'] = \"clickbait\"\n",
    "#Merging 2 files into one (*but with string type target values vs floats)\n",
    "Y_String = pd.concat([raw_click_copy, raw_nonclick_copy],sort=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data sorting 2/2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 27000 rows and 2 columns\n",
      "0.0    13501\n",
      "1.0    13499\n",
      "Name: target, dtype: int64\n",
      "\n",
      "Test: 5000 rows and 2 columns\n",
      "0.0    2500\n",
      "1.0    2500\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Split data into train & test\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged['feature'], merged['target'], test_size=5000, random_state=123,\n",
    "                                                    stratify=merged['target'])# Append sentiment back using indices\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)# Check dimensions\n",
    "print(f\"Train: {train.shape[0]} rows and {train.shape[1]} columns\")\n",
    "print(f\"{train['target'].value_counts()}\\n\")\n",
    "print(f\"Test: {test.shape[0]} rows and {test.shape[1]} columns\")\n",
    "print(test['target'].value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 500 columns\n",
      "\n",
      "This took 16.76 seconds\n",
      "\n",
      "Sample tokens: ['a', 'about', 'actually', 'adorable', 'af', 'afghan', 'afghanistan', 'after', 'again', 'against', 'age', 'aid', 'air', 'all', 'amazing', 'america', 'american', 'an', 'and', 'animal', 'announce', 'another', 'anyone', 'are', 'around', 'arrest', 'as', 'ask', 'at', 'attack', 'australia', 'australian', 'award', 'baby', 'back', 'bad', 'ban', 'bank', 'base', 'be', 'beautiful', 'been', 'before', 'begin', 'being', 'best', 'big', 'bill', 'billion', 'birth']\n",
      "\n",
      "Sample ignored tokens: ['commander', 'yule', 'easily', 'martyn', 'libra', 'wittle', 'emerisque', 'bremen', 'fortune', 'zendaya', 'suite', 'nicholas', 'sa', 'yummy', 'fassbender', 'kilimanjaro', 'pretender', 'slip', 'stripe', 'cursive', 'prince', 'moo', 'lup', 'angering', 'exchequer', 'temperament', 'frosh', 'offered', 'apec', 'eisenhower', 'scrapped', 'lift', 'sham', 'natalie', 'pascal', 'kiichi', 'zit', 'caitlin', 'nickel', 'harper', 'bind', 'sudden', 'jan', 'nifong', 'argentina', 'planner', 'shelton', 'einstein', 'collective', 'starkist']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gh/8lzt9_hj4kdc1xtzrnzmpd940000gp/T/ipykernel_9401/1743856629.py:13: DeprecationWarning: Sampling from a set deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  print(f\"Sample ignored tokens: {random.sample(ignored, 50)}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "              a  about  actually  adorable   af  afghan  afghanistan  \\\n0      0.000000    0.0       0.0       0.0  0.0     0.0          0.0   \n1      0.000000    0.0       0.0       0.0  0.0     0.0          0.0   \n2      0.000000    0.0       0.0       0.0  0.0     0.0          0.0   \n3      0.000000    0.0       0.0       0.0  0.0     0.0          0.0   \n4      0.316231    0.0       0.0       0.0  0.0     0.0          0.0   \n...         ...    ...       ...       ...  ...     ...          ...   \n31995  0.356680    0.0       0.0       0.0  0.0     0.0          0.0   \n31996  0.000000    0.0       0.0       0.0  0.0     0.0          0.0   \n31997  0.000000    0.0       0.0       0.0  0.0     0.0          0.0   \n31998  0.000000    0.0       0.0       0.0  0.0     0.0          0.0   \n31999  0.000000    0.0       0.0       0.0  0.0     0.0          0.0   \n\n          after  again  against  ...  worker  world  worst  would  year  \\\n0      0.000000    0.0      0.0  ...     0.0    0.0    0.0    0.0   0.0   \n1      0.000000    0.0      0.0  ...     0.0    0.0    0.0    0.0   0.0   \n2      0.000000    0.0      0.0  ...     0.0    0.0    0.0    0.0   0.0   \n3      0.000000    0.0      0.0  ...     0.0    0.0    0.0    0.0   0.0   \n4      0.236183    0.0      0.0  ...     0.0    0.0    0.0    0.0   0.0   \n...         ...    ...      ...  ...     ...    ...    ...    ...   ...   \n31995  0.000000    0.0      0.0  ...     0.0    0.0    0.0    0.0   0.0   \n31996  0.000000    0.0      0.0  ...     0.0    0.0    0.0    0.0   0.0   \n31997  0.000000    0.0      0.0  ...     0.0    0.0    0.0    0.0   0.0   \n31998  0.000000    0.0      0.0  ...     0.0    0.0    0.0    0.0   0.0   \n31999  0.000000    0.0      0.0  ...     0.0    0.0    0.0    0.0   0.0   \n\n           york       you  your  zealand  zodiac  \n0      0.000000  0.000000   0.0      0.0     0.0  \n1      0.000000  0.217385   0.0      0.0     0.0  \n2      0.000000  0.183638   0.0      0.0     0.0  \n3      0.387489  0.000000   0.0      0.0     0.0  \n4      0.000000  0.000000   0.0      0.0     0.0  \n...         ...       ...   ...      ...     ...  \n31995  0.000000  0.000000   0.0      0.0     0.0  \n31996  0.000000  0.000000   0.0      0.0     0.0  \n31997  0.000000  0.000000   0.0      0.0     0.0  \n31998  0.000000  0.000000   0.0      0.0     0.0  \n31999  0.000000  0.000000   0.0      0.0     0.0  \n\n[32000 rows x 500 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>about</th>\n      <th>actually</th>\n      <th>adorable</th>\n      <th>af</th>\n      <th>afghan</th>\n      <th>afghanistan</th>\n      <th>after</th>\n      <th>again</th>\n      <th>against</th>\n      <th>...</th>\n      <th>worker</th>\n      <th>world</th>\n      <th>worst</th>\n      <th>would</th>\n      <th>year</th>\n      <th>york</th>\n      <th>you</th>\n      <th>your</th>\n      <th>zealand</th>\n      <th>zodiac</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.217385</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.183638</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.387489</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.316231</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.236183</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>31995</th>\n      <td>0.356680</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31996</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31997</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31998</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>31999</th>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>32000 rows × 500 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analytics(vectoriser, X):\n",
    "    start_time = time()\n",
    "    print(f\"\\nThere are {vectoriser.fit_transform(X).shape[1]} columns\\n\")\n",
    "    end_time = time()\n",
    "    print(f\"This took {round((end_time-start_time),2)} seconds\\n\")\n",
    "    word_tokens = list(vectoriser.vocabulary_.keys())\n",
    "    word_tokens.sort()\n",
    "    print(f\"Sample tokens: {word_tokens[:50]}\\n\")\n",
    "    ignored = vectoriser.stop_words_\n",
    "    if len(ignored)==0:\n",
    "        print(\"No token is ignored.\")\n",
    "    elif len(ignored)>50:\n",
    "        print(f\"Sample ignored tokens: {random.sample(ignored, 50)}\")\n",
    "    else:\n",
    "        print(f\"Sample ignored tokens: {ignored}\")\n",
    "\n",
    "def custom_text_preprocessor(corpus):\n",
    "    tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "    word_tokens = tokenizer.tokenize(corpus)\n",
    "    pos_map = {'J': 'a', 'N': 'n', 'R': 'r', 'V': 'v'}\n",
    "    pos_tagz = pos_tag(word_tokens)\n",
    "    lem = WordNetLemmatizer()\n",
    "    word_tokens = [lem.lemmatize(t.lower(), pos=pos_map.get(p[0], 'v')) for t, p in pos_tagz]\n",
    "    return word_tokens\n",
    "\n",
    "#tfidf\n",
    "vectoriser = TfidfVectorizer(analyzer=custom_text_preprocessor, stop_words='english',\n",
    "                              max_df=.5, max_features=500)\n",
    "#printing analytics for X_train\n",
    "analytics(vectoriser, X_train)\n",
    "\n",
    "#fitting tfidf -- for X_train:\n",
    "X_train = vectoriser.fit_transform(X_train)\n",
    "X_train = pd.DataFrame.sparse.from_spmatrix(X_train)\n",
    "col_map = {v:k for k, v in vectoriser.vocabulary_.items()}\n",
    "for col in X_train.columns:\n",
    "    X_train.rename(columns={col: col_map[col]}, inplace=True)\n",
    "X_train\n",
    "\n",
    "\n",
    "#fitting tfidf -- for X test:\n",
    "# Fit to the data and transform to feature matrix\n",
    "X_test = vectoriser.fit_transform(X_test)\n",
    "# Convert sparse matrix to dataframe\n",
    "X_test = pd.DataFrame.sparse.from_spmatrix(X_test)\n",
    "# Save mapping on which index refers to which words\n",
    "col_map = {v:k for k, v in vectoriser.vocabulary_.items()}\n",
    "# Rename each column using the mapping\n",
    "for col in X_test.columns:\n",
    "    X_test.rename(columns={col: col_map[col]}, inplace=True)\n",
    "X_test\n",
    "\n",
    "\n",
    "#fitting tfidf -- for X:\n",
    "# Fit to the data and transform to feature matrix\n",
    "X = vectoriser.fit_transform(X)\n",
    "# Convert sparse matrix to dataframe\n",
    "X = pd.DataFrame.sparse.from_spmatrix(X)\n",
    "# Save mapping on which index refers to which words\n",
    "col_map = {v:k for k, v in vectoriser.vocabulary_.items()}\n",
    "# Rename each column using the mapping\n",
    "for col in X.columns:\n",
    "    X.rename(columns={col: col_map[col]}, inplace=True)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "K-nearest neighbors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non cv Training accuracy for comparison below:  0.9045555555555556\n",
      "non cv Testing accuracy for comparison below:  0.5286\n",
      "\n",
      "cv accuracy scores for train folds:  [0.8537037  0.86222222 0.87074074 0.85592593 0.86259259]\n",
      "cv accuracy scores *mean for train folds: 0.8610370370370373\n",
      "\t ± 0.5964688041243642\n",
      "\n",
      "cv sccuracy scores for test folds:  [0.721 0.71  0.721 0.675 0.729]\n",
      "cv accuracy scores *mean for test folds: 0.7112\n",
      "\t ± 1.9082976707002475\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "cv_scores_train = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "cv_scores_test = cross_val_score(knn, X_test, y_test, cv=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(\"non cv Training accuracy for comparison below: \", knn.score(X_train, y_train))\n",
    "print(\"non cv Testing accuracy for comparison below: \", knn.score(X_test, y_test))\n",
    "\n",
    "print(\"\\ncv accuracy scores for train folds: \", cv_scores_train)\n",
    "print('cv accuracy scores *mean for train folds: {}'.format(np.mean(cv_scores_train)))\n",
    "print(\"\\t ±\",(100*np.std(cv_scores_train)))\n",
    "print(\"\\ncv sccuracy scores for test folds: \", cv_scores_test)\n",
    "print('cv accuracy scores *mean for test folds: {}'.format(np.mean(cv_scores_test)))\n",
    "print(\"\\t ±\",(100*np.std(cv_scores_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Naïve Bayes - \"multinomial\" (using since popular for text classification)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv accuracy scores per train fold:  [0.92481481 0.92944444 0.93092593 0.93018519 0.92592593]\n",
      "cv *mean accuracy of train folds:  0.9282592592592593 ± 0.24\n",
      "\n",
      "cv accuracy scores per test fold:  [0.92  0.937 0.917 0.918 0.941]\n",
      "cv *mean accuracy of test folds:  0.9266 ± 1.02\n"
     ]
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "#quick testing - click bait headline to see if I get 'clickbait (1.) or nonclick (0.)'\n",
    "test_msg = ['If Disney Princesses Were From Florida']\n",
    "test_msg_counts = vectoriser.transform(test_msg)\n",
    "classifications = classifier.predict(test_msg_counts)\n",
    "classifications\n",
    "    #appears to be working! showing 1 as value which is labeled \"Yes\" (1.)\n",
    "\n",
    "#cross validation using a 5-fold cross validator\n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "scores_ = cross_val_score(classifier, X_test, y_test, cv=5)\n",
    "#printing accuracy per fold - train\n",
    "print(\"cv accuracy scores per train fold: \", scores)\n",
    "#printing mean accuracy of folds\n",
    "print(f\"cv *mean accuracy of train folds: \", scores.mean(), f\"± {100*np.std(scores):.2f}\")\n",
    "\n",
    "#printing accuracy per fold - test\n",
    "print(\"\\ncv accuracy scores per test fold: \", scores_)\n",
    "#printing mean accuracy of folds\n",
    "print(f\"cv *mean accuracy of test folds: \", scores_.mean(), f\"± {100*np.std(scores_):.2f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Multilayer perceptron"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean prediction accuracy:  0.9398\n",
      "Accuracy on training set  0.9302222222222222\n",
      "Accuracy on test set  0.6672\n",
      "\n",
      "Below metrics of interest from Cross Validation - train ~ \n",
      "\tKFold CV scores for:\n",
      "94.82%\n",
      "94.85%\n",
      "94.80%\n",
      "94.81%\n",
      "94.85%\n",
      "94.85%\n",
      "94.84%\n",
      "94.84%\n",
      "94.87%\n",
      "94.84%\n",
      "\n",
      "*Mean score of KFold CV: 94.84% ± 0.02%\n",
      "\n",
      "Below metrics of interest from Cross Validation - testing ~ \n",
      "\tKFold CV scores for:\n",
      "93.98%\n",
      "94.10%\n",
      "94.08%\n",
      "93.88%\n",
      "93.88%\n",
      "93.94%\n",
      "93.50%\n",
      "93.48%\n",
      "94.00%\n",
      "93.90%\n",
      "\n",
      "*Mean score of KFold CV: 93.87% ± 0.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Gutierya/Documents/School/Python_Schaffer/pythonProject2/venv/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (30) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#using 10 network configs (layer sizes below)\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=0, shuffle=True)\n",
    "classifier_mp = MLPClassifier(hidden_layer_sizes=10,max_iter=30,\n",
    "                              activation='logistic', learning_rate='adaptive')\n",
    "mp_scores = []\n",
    "mp_scores_test = []\n",
    "\n",
    "for train_indices, test_indices in kfold.split(X_train, y_train):\n",
    "    classifier_mp.fit(X_train, y_train)\n",
    "    mp_scores.append(classifier_mp.score(X_train, y_train))\n",
    "classifier_mp.fit(X_train, y_train)\n",
    "\n",
    "for train_indices, test_indices in kfold.split(X_test, y_test):\n",
    "    classifier_mp.fit(X_test, y_test)\n",
    "    mp_scores_test.append(classifier_mp.score(X_test, y_test))\n",
    "classifier_mp.fit(X_test, y_test)\n",
    "\n",
    "predictions_mp = classifier_mp.predict(X_test)\n",
    "scores_mp = accuracy_score(y_test, predictions_mp)#cross validation using a 5-fold cross validator\n",
    "print(\"Mean prediction accuracy: \", scores_mp)\n",
    "print(\"Accuracy on training set \", classifier.score(X_train, y_train))\n",
    "print(\"Accuracy on test set \", classifier.score(X_test, y_test))\n",
    "\n",
    "print(\"\\nBelow metrics of interest from Cross Validation - train ~ \")\n",
    "print('\\tKFold CV scores for:');[print(f'{(100*score):.2f}%') for score in mp_scores]\n",
    "print(f'\\n*Mean score of KFold CV: {100*np.mean(mp_scores):.2f}% ± {100*np.std(mp_scores):.2f}%')\n",
    "\n",
    "print(\"\\nBelow metrics of interest from Cross Validation - testing ~ \")\n",
    "print('\\tKFold CV scores for:');[print(f'{(100*s):.2f}%') for s in mp_scores_test]\n",
    "print(f'\\n*Mean score of KFold CV: {100*np.mean(mp_scores_test):.2f}% ± {100*np.std(mp_scores_test):.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Report:\n",
    "\n",
    "- Data representation used - Tfidf.\n",
    "- Model metric used for ranking - mean cross validation score.\n",
    "- Scoring of models on metric:\n",
    "    - Knn training = .86 ± .59 ; Knn test = .71 ± 1.9\n",
    "    - Naive Bayes train = .928 ± .24 ; Naive Bayes test = .926 ± 1.02\n",
    "    - Multilayer perceptron train = .94 ±.03 ; Multilayer perceptron test = .93 ± .11\n",
    "- Hyperparameter values that gave optimal results in cross validation were\n",
    "conservative values/ lower iterations surprisingly I had to test many values for example number of iterations, where the lower\n",
    "hyper parameters gave faster calculations and did not cause the program to break.\n",
    "- A way that the classifier could be used as a plugin for a web browser\n",
    "is, say an extension that filters news articles as spam or not or another example\n",
    "would be \"ad blocking\" via text classification of url, in order to decide to block\n",
    "the url/advise user of the possible scam website/ad."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}